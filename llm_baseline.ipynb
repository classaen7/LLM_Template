{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF File "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "# file_path : each PDF File path\n",
    "file_path = '/home/csh/workspace/DACON/finance_llm/data/train_source/ã€ŒFIS ì´ìŠˆ & í¬ì»¤ìŠ¤ã€ 22-3í˜¸ ã€Šì¬ì •ìœµìì‚¬ì—…ã€‹.pdf'\n",
    "\n",
    "doc = fitz.open(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fitz ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‹¤í—˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document('/home/csh/workspace/DACON/finance_llm/data/train_source/ã€ŒFIS ì´ìŠˆ & í¬ì»¤ìŠ¤ã€ 22-3í˜¸ ã€Šì¬ì •ìœµìì‚¬ì—…ã€‹.pdf')\n",
      "<class 'pymupdf.Document'>\n",
      "page_count :  9\n",
      "--------------------------------------------------\n",
      "get page : \n",
      "ISSUE & FOCUS\n",
      "FIS \n",
      "22-3í˜¸\n",
      "2022.11\n",
      ".\n",
      "  ë°œí–‰ì¸ ë°•ìš©ì£¼      ë°œí–‰ì²˜ 04637 ì„œìš¸íŠ¹ë³„ì‹œ  ì¤‘êµ¬ í‡´ê³„ë¡œ 10(ë‚¨ëŒ€ë¬¸ë¡œ5ê°€ 537) ë©”íŠ¸ë¡œíƒ€ì›Œ      T 02)6908-8200      F 02)6312-8959\n",
      "ì‘ì„± ë°•ì •ìˆ˜ ë¶€ì—°êµ¬ìœ„ì›, ìš°ìˆ˜ì—° ì—°êµ¬ì›    ê¸°íš\n",
      "Â·\n",
      "ì¡°ì • ì‹¬í˜œì¸ ê²°ì‚°ì •ë³´ë¶„ì„ë¶€ì¥\n",
      "ì¬ì •ìœµìì‚¬ì—…\n",
      "1  ë“¤ì–´ê°€ë©°\n",
      "2  ì¬ì •ìœµìì‚¬ì—…ì˜ ê°œë…ê³¼ ì˜ì˜\n",
      "3  2023ë…„ë„ ì˜ˆì‚°ì•ˆ ì¬ì •ìœµìì‚¬ì—… í˜„í™©\n",
      "4  ì¬ì •ìœµìì‚¬ì—…ì˜ ì£¼ìš” í˜„ì•ˆ\n",
      "5  ë‚˜ê°€ë©°\n",
      "\n",
      "--------------------------------------------------\n",
      "dtype of get_text(): <class 'str'>\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(doc)\n",
    "print(type(doc))\n",
    "print(\"page_count : \",end=\" \")\n",
    "print(doc.page_count)\n",
    "print(\"-\"*50)\n",
    "print(\"get page : \")\n",
    "print(doc[0].get_text())\n",
    "print(\"-\"*50)\n",
    "print(\"dtype of get_text(): \",end=\"\")\n",
    "print(type(doc[0].get_text()))\n",
    "print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitter\n",
    "PDFì—ì„œ ì–»ì€ ë¬¸ìì—´ ë°ì´í„°ë¥¼ ì²­í¬ ë‹¨ìœ„ë¡œ ë¶„í• "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "\n",
    "text = doc[0].get_text()\n",
    "\n",
    "# ëª¨ë“  í˜ì´ì§€ì˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "for page in doc:\n",
    "    text += page.get_text()\n",
    "    \n",
    "# í…ìŠ¤íŠ¸ë¥¼ chunkë¡œ ë¶„í• \n",
    "chunk_size=800\n",
    "chunk_overlap=50\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "chunk_temp = splitter.split_text(text)\n",
    "# Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ ìƒì„±\n",
    "chunks = [Document(page_content=t) for t in chunk_temp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ISSUE & FOCUS\\nFIS \\n22-3í˜¸\\n2022.11\\n.\\n  ë°œí–‰ì¸ ë°•ìš©ì£¼      ë°œí–‰ì²˜ 04637 ì„œìš¸íŠ¹ë³„ì‹œ  ì¤‘êµ¬ í‡´ê³„ë¡œ 10(ë‚¨ëŒ€ë¬¸ë¡œ5ê°€ 537) ë©”íŠ¸ë¡œíƒ€ì›Œ      T 02)6908-8200      F 02)6312-8959\\nì‘ì„± ë°•ì •ìˆ˜ ë¶€ì—°êµ¬ìœ„ì›, ìš°ìˆ˜ì—° ì—°êµ¬ì›    ê¸°íš\\nÂ·\\nì¡°ì • ì‹¬í˜œì¸ ê²°ì‚°ì •ë³´ë¶„ì„ë¶€ì¥\\nì¬ì •ìœµìì‚¬ì—…\\n1  ë“¤ì–´ê°€ë©°\\n2  ì¬ì •ìœµìì‚¬ì—…ì˜ ê°œë…ê³¼ ì˜ì˜\\n3  2023ë…„ë„ ì˜ˆì‚°ì•ˆ ì¬ì •ìœµìì‚¬ì—… í˜„í™©\\n4  ì¬ì •ìœµìì‚¬ì—…ì˜ ì£¼ìš” í˜„ì•ˆ\\n5  ë‚˜ê°€ë©°\\nISSUE & FOCUS\\nFIS \\n22-3í˜¸\\n2022.11\\n.\\n  ë°œí–‰ì¸ ë°•ìš©ì£¼      ë°œí–‰ì²˜ 04637 ì„œìš¸íŠ¹ë³„ì‹œ  ì¤‘êµ¬ í‡´ê³„ë¡œ 10(ë‚¨ëŒ€ë¬¸ë¡œ5ê°€ 537) ë©”íŠ¸ë¡œíƒ€ì›Œ      T 02)6908-8200      F 02)6312-8959\\nì‘ì„± ë°•ì •ìˆ˜ ë¶€ì—°êµ¬ìœ„ì›, ìš°ìˆ˜ì—° ì—°êµ¬ì›    ê¸°íš\\nÂ·\\nì¡°ì • ì‹¬í˜œì¸ ê²°ì‚°ì •ë³´ë¶„ì„ë¶€ì¥\\nì¬ì •ìœµìì‚¬ì—…\\n1  ë“¤ì–´ê°€ë©°\\n2  ì¬ì •ìœµìì‚¬ì—…ì˜ ê°œë…ê³¼ ì˜ì˜\\n3  2023ë…„ë„ ì˜ˆì‚°ì•ˆ ì¬ì •ìœµìì‚¬ì—… í˜„í™©\\n4  ì¬ì •ìœµìì‚¬ì—…ì˜ ì£¼ìš” í˜„ì•ˆ\\n5  ë‚˜ê°€ë©°\\n02\\n03\\nFIS    ISSUE & FOCUS \\n                   ë“¤ì–´ê°€ë©°\\nISSUE   ì™œ ì¬ì •ìœµìì‚¬ì—…ì— ì£¼ëª©í•˜ëŠ”ê°€?\\n\\x03\\nì¬ì •ìœµìì‚¬ì—…ì€ ì •ë¶€ê°€ ìê¸ˆì„ ë¯¼ê°„ì˜ ì‚¬ì  ê²½ì œì£¼ì²´ì— ëŒ€í•´ ëŒ€ì¶œí•´ ì£¼ê³  íšŒìˆ˜í•˜ëŠ” í™œë™ì„ ë§í•˜ë©°, ì •ë¶€\\nì˜ ë‹¤ì–‘í•œ ê¸ˆìœµí™œë™ ì¤‘ ì§ì ‘ëŒ€ì¶œê³¼ ì „ëŒ€ ë°©ì‹ì— í•´ë‹¹(ì¬ë¬´ë¶€, 1993; í•œêµ­ì¬ì •ì •ë³´ì›, 2019)\\n    - \\x03\\nì •ë¶€ì˜ ê¸ˆìœµí™œë™ì€ ì§ì ‘ìœµìÂ·ì „ëŒ€ ì™¸ì—ë„ ê¸ˆë¦¬ ì°¨ì•¡ì— ëŒ€í•œ ë³´ìƒ(ì´ì°¨ë³´ì „), ê¸°ì—…ì˜ ì‹ ìš©ì‹¬ì‚¬ ë° ë³´ì¦(ì‹ ìš©ë³´'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector DB\n",
    "ë§Œë“  Document ë°ì´í„°ë“¤ì„ Vector DBì— ì €ì¥\n",
    "\n",
    "- **FAISS : íŒŒì‹œìŠ¤ (Facebook AI Similarity Search)**\n",
    "\n",
    "ëŒ€ëŸ‰ì˜ ê³ ì°¨ì› ë²¡í„°ì—ì„œ ìœ ì‚¬ì„± ê²€ìƒ‰ ë° í´ëŸ¬ìŠ¤í„°ë§ì„ ë¹ ë¥´ê³  íš¨ìœ¨ì ìœ¼ë¡œ ìˆ˜í–‰<br>\n",
    "\n",
    "\n",
    "`from_documents` í´ë˜ìŠ¤ ë©”ì„œë“œëŠ” ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ì™€ ì„ë² ë”© í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ FAISS ë²¡í„° ì €ì¥ì†Œë¥¼ ìƒì„±<br>\n",
    "<íŒŒë¼ë¯¸í„°>ë¡œ ì„ë² ë”© í•¨ìˆ˜(Embeddings)ê³¼ ë°ì´í„°(List[Document]) ì…ë ¥ <br>\n",
    "<ë°˜í™˜ê°’> `VectorStore`: ë¬¸ì„œì™€ ì„ë² ë”©ìœ¼ë¡œ ì´ˆê¸°í™”ëœ ë²¡í„° ì €ì¥ì†Œ ì¸ìŠ¤í„´ìŠ¤\n",
    "\n",
    "- **VectorStoreë€**\n",
    "ìì—°ì–´ ì²˜ë¦¬(NLP) \n",
    "\n",
    "ë° ê¸°ê³„ í•™ìŠµ ë¶„ì•¼ì—ì„œ ë²¡í„° ê²€ìƒ‰ì„ ìœ„í•œ íš¨ìœ¨ì ì¸ ë°ì´í„° ì €ì¥ ë° ê²€ìƒ‰ ì‹œìŠ¤í…œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/csh/workspace/LLM_study/.venv/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:141: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "/home/csh/workspace/LLM_study/.venv/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "def create_vector_db(chunks, model_path=\"intfloat/multilingual-e5-small\"):\n",
    "    \"\"\"FAISS DB ìƒì„±\"\"\"\n",
    "    # ì„ë² ë”© ëª¨ë¸ ì„¤ì •\n",
    "    model_kwargs = {'device': 'cuda'}\n",
    "    encode_kwargs = {'normalize_embeddings': True}\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=model_path,\n",
    "        model_kwargs=model_kwargs,\n",
    "        encode_kwargs=encode_kwargs\n",
    "    )\n",
    "    # FAISS DB ìƒì„± ë° ë°˜í™˜\n",
    "    db = FAISS.from_documents(chunks, embedding=embeddings)\n",
    "    return db\n",
    "\n",
    "\n",
    "db = create_vector_db(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x7f27389ab220>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.save_local(\"./save_fasiss\") # ì´ë ‡ê²Œ ì €ì¥í•˜ê³ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<langchain_community.docstore.in_memory.InMemoryDocstore object at 0x7f280a9f7cd0>, {0: 'ee550deb-713a-4c0a-a52c-0d20facf956f', 1: '24e7305e-b957-4210-9b96-514cd651cc21', 2: 'e4c88563-6eb3-41af-8eed-74158641d0ec', 3: '3ec90013-f679-4f1c-859c-b1b9acd3c5b8', 4: 'de856b56-962b-40d2-a6b6-3ce9ed6eb9c4', 5: '48ab05d9-7b13-4955-8a11-c9e43278cf57', 6: '01926be0-d242-4bc3-b5b2-981ac80cac93', 7: '26abb1fb-db23-44de-bed9-2342ac35df5c', 8: '8223aee2-426a-4059-b37a-77abef43b1bf', 9: 'a45855cf-0201-4fc7-a6bd-30d4c003b5d9', 10: 'e64f3fb3-59fe-4101-94e3-aa92f7ccdaab', 11: '07da717a-3873-467c-8fa4-a1717000dc1c', 12: '6f3d2ca0-21f3-4459-86c7-839dbb2a6b07', 13: '42aa123c-8bf5-4329-add4-37bee2e737e1', 14: 'd508b7de-e08c-4000-8650-1d5fcffdf2e3', 15: '0b6060be-95f9-4e23-85f6-6bb7c208fb94', 16: '0468411a-3eb7-4f1a-b5e8-5643ca3a36e0', 17: '8545f207-2d36-4ad6-9708-1abf1853ac3c', 18: '5f664cde-af29-46b8-b072-48fb74903666', 19: 'e7ad4ad5-2701-4500-8689-8fd599fe61da', 20: 'e2bfc7af-b8f6-4605-926b-43f5f54c02aa', 21: 'd0f51b02-a847-4676-abb6-839b96c5f3a1', 22: 'fc782019-b37a-4575-9205-96874bd4b98e', 23: '95a558f9-05e6-46a1-9cb5-5e9f3d6ea5ce', 24: 'afde14d2-47e1-46ec-800c-3daf4a12b34a'})\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"./save_fasiss/index.pkl\",\"rb\") as fs:\n",
    "    data = pickle.load(fs)\n",
    "    print(data)\n",
    "# ë¶ˆëŸ¬ë³´ë©´ ì„ë² ë”©ëœ ê°’ìœ¼ë¡œ ì €ì¥í•˜ê³  ìˆìŒ \n",
    "# -> ì´ê±° ë‹¤ì‹œ ëŒë¦¬ëŠ”ê±°ë¡œ ë°›ìœ¼ë ¤ë©´ load_local + embedding_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retriever\n",
    "ë¦¬íŠ¸ë¦¬ë²„(retriever)ëŠ” ì •ë³´ ê²€ìƒ‰ ì‹œìŠ¤í…œì—ì„œ ì¤‘ìš”í•œ ì—­í• ì„ í•˜ëŠ” ì»´í¬ë„ŒíŠ¸ë¡œ, ì£¼ì–´ì§„ ì¿¼ë¦¬ì— ëŒ€í•´ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ê³  ë°˜í™˜í•˜ëŠ” ê¸°ëŠ¥ì„ ë‹´ë‹¹ = ê°„ë‹¨í•˜ê²Œ **ê²€ìƒ‰ ë„êµ¬**\n",
    "\n",
    "\"Vector stores can be used as the backbone of a retriever\"<br>\n",
    "\"Retrievers accept a string query as input and return a list of Document's as output.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retriever ìƒì„±\n",
    "retriever = db.as_retriever(search_type=\"mmr\", # Maximum Marginal Relevance\n",
    "                  search_kwargs={'k': 3, 'fetch_k': 8}) \n",
    "                # k :ê²€ìƒ‰ ì‹œ ì„ íƒí•  ìµœëŒ€ ê²°ê³¼ ìˆ˜ (ìµœì¢…)\n",
    "                # fetch_k : ê²€ìƒ‰ì„ ìœ„í•´ ê°€ì ¸ì˜¬ ê²°ê³¼ ìˆ˜ (í›„ë³´)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140631844762432"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7fe76726b340>, search_type='mmr', search_kwargs={'k': 3, 'fetch_k': 8})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever\n",
    "# ë‹¤ë¥¸ ì €ì¥ì†Œ (ex Chroma)ë„ langchainì˜ í•˜ìœ„ ë¼ì´ë¸ŒëŸ¬ë¦¬ì— ì €ì¥ë˜ì–´ìˆìŒ\n",
    "# ë”°ë¼ì„œ tagsì— ì–´ë–¤ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í–ˆëŠ”ì§€ ë‚˜ì™€ìˆê³ \n",
    "# vectorstoreì—ëŠ” FAISS ê°ì²´ê°€ ë‹´ê²¨ìˆìŒ\n",
    "# search_typeê³¼ search_kwargsì— "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(id(db) == id(retriever.vectorstore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "<class 'langchain_core.documents.base.Document'>\n",
      "355\n",
      "303\n",
      "-1\n"
     ]
    }
   ],
   "source": [
    "outputs = retriever.invoke(\"ì½”ë¡œë‚˜ê±¸ë ¤ì„œ ê¸°ì¹¨ì´ ë‚˜ì™€\")\n",
    "print(len(outputs))\n",
    "print(type((outputs[0])))\n",
    "print(outputs[0].page_content.find(\"ì½”ë¡œë‚˜\"))\n",
    "print(outputs[1].page_content.find(\"ì½”ë¡œë‚˜\"))\n",
    "print(outputs[2].page_content.find(\"ì½”ë¡œë‚˜\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remind\n",
    "\n",
    "ì§€ê¸ˆê¹Œì§€ \n",
    "1. PDF íŒŒì¼ì„ í†µí•´ ë¨¼ì € ë¬¸ìì—´ë¡œ ë§Œë“¤ì—ˆê³ \n",
    "2. ê·¸ ë¬¸ìì—´ì„ íŠ¹ì • ë‹¨ìœ„ë¡œ(ì²­í¬) Spilt í–ˆê³ \n",
    "3. ë‚˜ëˆ ì§„ ë°ì´í„°ë¥¼ Embedding ëª¨ë¸ì„ í†µí•´ Vectorë¡œ ë§Œë“¤ë©´ì„œ\n",
    "4. FAISSë¥¼ í†µí•´ Vector DBì— ì €ì¥í–ˆìŒ\n",
    "5. ë§ˆì§€ë§‰ìœ¼ë¡œ Vector DBë¥¼ í†µí•´ Retriever ê°ì²´ë¥¼ ë§Œë“¤ì–´ì„œ (ì§ˆë¬¸-> ìœ ì‚¬ë„ë†’ì€ ëŒ€ë‹µ í›„ë³´)ë¥¼ ê°€ëŠ¥í•˜ê²Œí•¨\n",
    "\n",
    "\n",
    "ì¤‘ê°„ ì¤‘ê°„ì— ë°”ê¿€ìˆ˜ ìˆëŠ” ê²ƒë“¤\n",
    "- **PDFë¥¼ ì–´ë–¤ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í†µí•´ì„œ ë¬¸ìì—´ë¡œ ë°”ê¿€ì§€**\n",
    "- **ì–´ë–¤ ë‹¨ìœ„ë¡œ ë¬¸ì„œë¥¼ ë¶„ë¦¬í• ì§€**\n",
    "- **ì–´ë–¤ ì„ë² ë”© ëª¨ë¸ì„ ì‚¬ìš©í• ì§€**\n",
    "- **ì–´ë–¤ Vector DBë¥¼ ì‚¬ìš©í• ì§€**\n",
    "- **ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ Vectorë¥¼ ì°¾ì„ì§€ (íŒŒë¼ë¯¸í„°)**\n",
    "\n",
    "### ì¶”ê°€ ìƒê°\n",
    "PDFì˜ í…Œì´ë¸”, í‘œ, ê·¸ë¦¼ìœ¼ë¡œ ë˜ì–´ìˆëŠ” ìë£ŒëŠ” ë¶ˆëŸ¬ì˜¬ ë•Œ ì¸ì‹í•˜ê¸° ì–´ë ¤ìš´ ê²½ìš°ê°€ ì¡´ì¬í•¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import unicodedata\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import fitz  # PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf(file_path, chunk_size=800, chunk_overlap=50):\n",
    "    \"\"\"PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ í›„ chunk ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ê¸°\"\"\"\n",
    "    # PDF íŒŒì¼ ì—´ê¸°\n",
    "    doc = fitz.open(file_path)\n",
    "    text = ''\n",
    "    # ëª¨ë“  í˜ì´ì§€ì˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    # í…ìŠ¤íŠ¸ë¥¼ chunkë¡œ ë¶„í• \n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    chunk_temp = splitter.split_text(text)\n",
    "    # Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ ìƒì„±\n",
    "    chunks = [Document(page_content=t) for t in chunk_temp]\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def create_vector_db(chunks, model_path=\"intfloat/multilingual-e5-small\"):\n",
    "    \"\"\"FAISS DB ìƒì„±\"\"\"\n",
    "    # ì„ë² ë”© ëª¨ë¸ ì„¤ì •\n",
    "    model_kwargs = {'device': 'cuda'}\n",
    "    encode_kwargs = {'normalize_embeddings': True}\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=model_path,\n",
    "        model_kwargs=model_kwargs,\n",
    "        encode_kwargs=encode_kwargs\n",
    "    )\n",
    "    # FAISS DB ìƒì„± ë° ë°˜í™˜\n",
    "    db = FAISS.from_documents(chunks, embedding=embeddings)\n",
    "    return db\n",
    "\n",
    "def normalize_path(path):\n",
    "    \"\"\"ê²½ë¡œ ìœ ë‹ˆì½”ë“œ ì •ê·œí™”\"\"\"\n",
    "    return unicodedata.normalize('NFC', path)\n",
    "\n",
    "\n",
    "def process_pdfs_from_dataframe(df, base_directory):\n",
    "    \"\"\"ë”•ì…”ë„ˆë¦¬ì— pdfëª…ì„ í‚¤ë¡œí•´ì„œ DB, retriever ì €ì¥\"\"\"\n",
    "    pdf_databases = {}\n",
    "    unique_paths = df['Source_path'].unique()\n",
    "    # unique_paths = unique_paths[:2]\n",
    "    \n",
    "    for path in tqdm(unique_paths, desc=\"Processing PDFs\"):\n",
    "        # ê²½ë¡œ ì •ê·œí™” ë° ì ˆëŒ€ ê²½ë¡œ ìƒì„±\n",
    "        normalized_path = normalize_path(path)\n",
    "        full_path = os.path.normpath(os.path.join(base_directory, normalized_path.lstrip('./'))) if not os.path.isabs(normalized_path) else normalized_path\n",
    "        \n",
    "        pdf_title = os.path.splitext(os.path.basename(full_path))[0]\n",
    "        print(f\"Processing {pdf_title}...\")\n",
    "        \n",
    "        # PDF ì²˜ë¦¬ ë° ë²¡í„° DB ìƒì„±\n",
    "        chunks = process_pdf(full_path)\n",
    "        db = create_vector_db(chunks)\n",
    "        \n",
    "        # Retriever ìƒì„±\n",
    "        retriever = db.as_retriever(search_type=\"mmr\", \n",
    "                                    search_kwargs={'k': 3, 'fetch_k': 8})\n",
    "        \n",
    "        # ê²°ê³¼ ì €ì¥\n",
    "        pdf_databases[pdf_title] = {\n",
    "                'db': db,\n",
    "                'retriever': retriever\n",
    "        }\n",
    "    return pdf_databases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:   0%|          | 0/9 [00:00<?, ?it/s]/home/csh/workspace/LLM_study/.venv/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:141: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ì¤‘ì†Œë²¤ì²˜ê¸°ì—…ë¶€_í˜ì‹ ì°½ì—…ì‚¬ì—…í™”ìê¸ˆ(ìœµì)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  11%|â–ˆ         | 1/9 [00:05<00:47,  5.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ë³´ê±´ë³µì§€ë¶€_ë¶€ëª¨ê¸‰ì—¬(ì˜ì•„ìˆ˜ë‹¹) ì§€ì›...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  22%|â–ˆâ–ˆâ–       | 2/9 [00:09<00:32,  4.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ë³´ê±´ë³µì§€ë¶€_ë…¸ì¸ì¥ê¸°ìš”ì–‘ë³´í—˜ ì‚¬ì—…ìš´ì˜...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  33%|â–ˆâ–ˆâ–ˆâ–      | 3/9 [00:13<00:25,  4.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ì‚°ì—…í†µìƒìì›ë¶€_ì—ë„ˆì§€ë°”ìš°ì²˜...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:16<00:19,  3.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing êµ­í† êµí†µë¶€_í–‰ë³µì£¼íƒì¶œì...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:20<00:14,  3.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ã€ŒFIS ì´ìŠˆ & í¬ì»¤ìŠ¤ã€ 22-4í˜¸ ã€Šì¤‘ì•™-ì§€ë°© ê°„ ì¬ì •ì¡°ì •ì œë„ã€‹...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:23<00:10,  3.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ã€ŒFIS ì´ìŠˆ & í¬ì»¤ìŠ¤ã€ 23-2í˜¸ ã€Ší•µì‹¬ì¬ì •ì‚¬ì—… ì„±ê³¼ê´€ë¦¬ã€‹...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:26<00:06,  3.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ã€ŒFIS ì´ìŠˆ&í¬ì»¤ìŠ¤ã€ 22-2í˜¸ ã€Šì¬ì •ì„±ê³¼ê´€ë¦¬ì œë„ã€‹...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:29<00:03,  3.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ã€ŒFIS ì´ìŠˆ & í¬ì»¤ìŠ¤ã€(ì‹ ê·œ) í†µê¶Œ ì œ1í˜¸ ã€Šìš°ë°œë¶€ì±„ã€‹...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:33<00:00,  3.72s/it]\n"
     ]
    }
   ],
   "source": [
    "base_directory = './data' # Your Base Directory\n",
    "df = pd.read_csv('./data/test.csv')\n",
    "pdf_databases = process_pdfs_from_dataframe(df, base_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ëª¨ë¸ í•™ìŠµ\n",
    "\n",
    "ê°œë°œ í™˜ê²½ì´ ì¢‹ì§€ ì•Šì•„ì„œ ì‘ì€ ëª¨ë¸ë¡œ ì‹¤í—˜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/csh/workspace/LLM_study/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import unicodedata\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    pipeline,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from accelerate import Accelerator\n",
    "\n",
    "# Langchain ê´€ë ¨\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ëª¨ë¸ í˜¸ì¶œ\n",
    "\n",
    "ê°œë°œí™˜ê²½ì˜ ë¬¸ì œë¡œ ì–‘ìí™” & 16bit(ëª¨ë¸ íŠ¹ì„±ìƒ bf16)ë¡œ ëª¨ë¸ í˜¸ì¶œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/csh/workspace/LLM_study/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.13s/it]\n"
     ]
    }
   ],
   "source": [
    "# ëª¨ë¸ ID \n",
    "model_id = \"EleutherAI/polyglot-ko-1.3b\"\n",
    "\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#         load_in_4bit=True,\n",
    "#         bnb_4bit_use_double_quant=True,\n",
    "#         bnb_4bit_quant_type=\"nf4\",\n",
    "#         bnb_4bit_compute_dtype=torch.bfloat16\n",
    "#     )\n",
    "\n",
    "# í† í¬ë‚˜ì´ì € ë¡œë“œ ë° ì„¤ì •\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# ëª¨ë¸ ë¡œë“œ ë° ì–‘ìí™” ì„¤ì • ì ìš©\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "trainì„ ìœ„í•œ datasetì€ ì•„ë˜ì™€ ê°™ì€ í˜•íƒœë¡œ í•„ìˆ˜ columnì´ ì¡´ì¬í•¨\n",
    "\n",
    "ë”°ë¼ì„œ ì ì ˆí•œ tokenizer í•¨ìˆ˜ë¥¼ í†µí•´ ë³€í™˜í•´ ì¤˜ì•¼í•¨\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('csv', data_files='/home/csh/workspace/LLM_study/data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['SAMPLE_ID', 'Source', 'Source_path', 'Question', 'Answer'], dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('/home/csh/workspace/LLM_study/data/train.csv').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['SAMPLE_ID', 'Source', 'Source_path', 'Question', 'Answer'],\n",
       "        num_rows: 496\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['SAMPLE_ID', 'Source', 'Source_path', 'Question', 'Answer'],\n",
       "        num_rows: 446\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['SAMPLE_ID', 'Source', 'Source_path', 'Question', 'Answer'],\n",
       "        num_rows: 50\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset['train'].train_test_split(test_size=0.1)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer\n",
    "\n",
    "- **input_ids**\n",
    "\n",
    "ëª¨ë¸ì— ì…ë ¥ë˜ëŠ” ë¬¸ì¥ì˜ ê° í† í°ì„ ë‚˜íƒ€ë‚´ëŠ” ì •ìˆ˜ ì¸ë±ìŠ¤ -> ëª¨ë¸ì˜ vocabularyì™€ ë§¤í•‘ë˜ì–´ìˆìŒ \n",
    "\n",
    "**token_type_ids**\n",
    "\n",
    "ë¬¸ì¥ ìŒì„ ì²˜ë¦¬í•  ë•Œ ê° ë¬¸ì¥ì„ êµ¬ë¶„í•˜ëŠ” ë° ì‚¬ìš©. ì¼ë°˜ì ìœ¼ë¡œ ë‘ ë¬¸ì¥ì´ ì…ë ¥ìœ¼ë¡œ ì£¼ì–´ì§€ëŠ” ê²½ìš°(ì˜ˆ: ì§ˆë¬¸-ì‘ë‹µ ìŒ, ë¬¸ì¥ ìŒ ë¶„ë¥˜ ë“±), ì´ ë°°ì—´ì€ ê° í† í°ì´ ì–´ë–¤ ë¬¸ì¥ì— ì†í•˜ëŠ”ì§€ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\n",
    "\n",
    "**attention_mask**\n",
    "\n",
    "íŒ¨ë”©(padding) í† í°ê³¼ ì‹¤ì œ ì…ë ¥ í† í°ì„ êµ¬ë¶„í•˜ëŠ” ë° ì‚¬ìš©<br> : 1ì€ ì‹¤ì œ ì…ë ¥ í† í° / 0ì€ íŒ¨ë”© í† í°\n",
    "ëª¨ë¸ì€ íŒ¨ë”© í† í°ì„ ë¬´ì‹œí•˜ê³  ì‹¤ì œ ì…ë ¥ í† í°ë§Œì„ ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/446 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 446/446 [00:00<00:00, 1811.01 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<00:00, 2033.56 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_dataset(dataset):\n",
    "    re_dataset = tokenizer(dataset['Question'], padding=True, truncation=True)\n",
    "    label = tokenizer(dataset['Answer'], padding=True, truncation=True)\n",
    "    re_dataset['labels'] = label['input_ids']\n",
    "    return re_dataset\n",
    "\n",
    "dataset = dataset.map(tokenize_dataset, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['SAMPLE_ID', 'Source', 'Source_path', 'Question', 'Answer', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 446\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['SAMPLE_ID', 'Source', 'Source_path', 'Question', 'Answer', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 50\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2ë¡œ íŒ¨ë”©ëœê±°ëŠ” ì—¬ê¸°ì„  <|endoftext|>\n",
    "tokenizer.decode(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoRA\n",
    "\n",
    "low rankë¡œ ë³€í™˜í•´ì£¼ëŠ” ê³¼ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from transformers import DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXForCausalLM(\n",
       "  (gpt_neox): GPTNeoXModel(\n",
       "    (embed_in): Embedding(30080, 2048)\n",
       "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (attention): GPTNeoXSdpaAttention(\n",
       "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (embed_out): Linear(in_features=2048, out_features=30080, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoRA ì ìš© í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6,291,456 || all params: 1,338,101,760 || trainable%: 0.4702\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "p_model = get_peft_model(model, lora_config)\n",
    "p_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # Masked Language Modelì´ ì•„ë‹Œ Causal Language Modelì˜ ê²½ìš° Falseë¡œ ì„¤ì •\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/csh/workspace/LLM_study/.venv/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',              # ê²°ê³¼ë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬\n",
    "    evaluation_strategy=\"steps\",         # í‰ê°€ ì „ëµì„ \"steps\"ë¡œ ì„¤ì •\n",
    "    eval_steps=30,                      # í‰ê°€ ê°„ê²© (ìŠ¤í… ë‹¨ìœ„)\n",
    "    learning_rate=2e-5,                  # í•™ìŠµë¥ \n",
    "    per_device_train_batch_size=4,       # í›ˆë ¨ ë°°ì¹˜ í¬ê¸°\n",
    "    per_device_eval_batch_size=4,        # í‰ê°€ ë°°ì¹˜ í¬ê¸°\n",
    "    num_train_epochs=3,                  # ì—í­ ìˆ˜\n",
    "    weight_decay=0.01,                   # ê°€ì¤‘ì¹˜ ê°ì†Œ\n",
    "    logging_dir='./logs',                # ë¡œê·¸ ë””ë ‰í† ë¦¬\n",
    "    logging_steps=10,                    # ë¡œê·¸ ê¸°ë¡ ê°„ê²©\n",
    "    prediction_loss_only=True            # ìƒì„± ëª¨ë¸ì—ì„œëŠ” ì¼ë°˜ì ìœ¼ë¡œ lossë§Œì„ ì˜ˆì¸¡\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Generateì˜ ê²½ìš° SFT(Supervised-Fine-Tuning)Trainer ì‚¬ìš©ì´ ìš©ì´í•¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/csh/workspace/LLM_study/.venv/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/csh/workspace/LLM_study/.venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:289: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,                         # ëª¨ë¸\n",
    "    args=training_args,                  # í›ˆë ¨ ì¸ìˆ˜\n",
    "    train_dataset=dataset['train'],   # í›ˆë ¨ ë°ì´í„°ì…‹\n",
    "    eval_dataset=dataset['test'] ,  # í‰ê°€ ë°ì´í„°ì…‹\n",
    "    tokenizer=tokenizer,                 # í† í¬ë‚˜ì´ì €\n",
    "    data_collator=data_collator,           # ë°ì´í„° ì½œë ˆì´í„°\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='336' max='336' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [336/336 23:14, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.990600</td>\n",
       "      <td>2.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.751600</td>\n",
       "      <td>2.770000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.740600</td>\n",
       "      <td>2.680625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.218000</td>\n",
       "      <td>2.636875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.356300</td>\n",
       "      <td>2.601875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>2.180500</td>\n",
       "      <td>2.568125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>2.305500</td>\n",
       "      <td>2.557500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>2.098400</td>\n",
       "      <td>2.548750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>2.182800</td>\n",
       "      <td>2.545000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.371100</td>\n",
       "      <td>2.543750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>2.299200</td>\n",
       "      <td>2.543750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=336, training_loss=2.4826543898809526, metrics={'train_runtime': 1403.1646, 'train_samples_per_second': 0.954, 'train_steps_per_second': 0.239, 'total_flos': 499663657156608.0, 'train_loss': 2.4826543898809526, 'epoch': 3.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/csh/workspace/LLM_study/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:03<00:00,  1.23s/it]\n"
     ]
    }
   ],
   "source": [
    "# ëª¨ë¸ ID \n",
    "model_id = \"EleutherAI/polyglot-ko-1.3b\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "\n",
    "# í† í¬ë‚˜ì´ì € ë¡œë“œ ë° ì„¤ì •\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# ëª¨ë¸ ë¡œë“œ ë° ì–‘ìí™” ì„¤ì • ì ìš©\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/csh/workspace/LLM_study/.venv/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:141: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 0.3. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFacePipeline`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# HuggingFacePipeline ê°ì²´ ìƒì„±\n",
    "text_generation_pipeline = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    temperature=0.2,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=128,\n",
    ")\n",
    "\n",
    "hf = HuggingFacePipeline(pipeline=text_generation_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_string(s):\n",
    "    \"\"\"ìœ ë‹ˆì½”ë“œ ì •ê·œí™”\"\"\"\n",
    "    return unicodedata.normalize('NFC', s)\n",
    "\n",
    "def format_docs(docs):\n",
    "    \"\"\"ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í¬ë§·íŒ…\"\"\"\n",
    "    context = \"\"\n",
    "    for doc in docs:\n",
    "        context += doc.page_content\n",
    "        context += '\\n'\n",
    "    return context\n",
    "\n",
    "# ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”\n",
    "results = []\n",
    "\n",
    "# DataFrameì˜ ê° í–‰ì— ëŒ€í•´ ì²˜ë¦¬\n",
    "for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Answering Questions\"):\n",
    "    # ì†ŒìŠ¤ ë¬¸ìì—´ ì •ê·œí™”\n",
    "    source = normalize_string(row['Source'])\n",
    "    question = row['Question']\n",
    "\n",
    "    # ì •ê·œí™”ëœ í‚¤ë¡œ ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰\n",
    "    normalized_keys = {normalize_string(k): v for k, v in pdf_databases.items()}\n",
    "    retriever = normalized_keys[source]['retriever']\n",
    "\n",
    "    # RAG ì²´ì¸ êµ¬ì„±\n",
    "    template = \"\"\"\n",
    "    ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”:\n",
    "    {context}\n",
    "\n",
    "    ì§ˆë¬¸: {question}\n",
    "\n",
    "    ë‹µë³€:\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "    # RAG ì²´ì¸ ì •ì˜\n",
    "    rag_chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | hf\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    # ë‹µë³€ ì¶”ë¡ \n",
    "    print(f\"Question: {question}\")\n",
    "    full_response = rag_chain.invoke(question)\n",
    "\n",
    "    print(f\"Answer: {full_response}\\n\")\n",
    "\n",
    "    # ê²°ê³¼ ì €ì¥\n",
    "    results.append({\n",
    "        \"Source\": row['Source'],\n",
    "        \"Source_path\": row['Source_path'],\n",
    "        \"Question\": question,\n",
    "        \"Answer\": full_response\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì œì¶œìš© ìƒ˜í”Œ íŒŒì¼ ë¡œë“œ\n",
    "submit_df = pd.read_csv(\"./data/sample_submission.csv\")\n",
    "\n",
    "# ìƒì„±ëœ ë‹µë³€ì„ ì œì¶œ DataFrameì— ì¶”ê°€\n",
    "submit_df['Answer'] = [item['Answer'] for item in results]\n",
    "submit_df['Answer'] = submit_df['Answer'].fillna(\"Nan\")     # ëª¨ë¸ì—ì„œ ë¹ˆ ê°’ (NaN) ìƒì„± ì‹œ ì±„ì ì— ì˜¤ë¥˜ê°€ ë‚  ìˆ˜ ìˆìŒ [ ì£¼ì˜ ]\n",
    "\n",
    "# ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥\n",
    "submit_df.to_csv(\"./data/baseline_submission.csv\", encoding='UTF-8-sig', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-study-YfWbGZHI-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
